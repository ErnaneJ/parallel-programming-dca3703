# üìò Produto Matriz-Vetor com MPI

## üß© Objetivo

Este projeto implementa a multiplica√ß√£o de uma matriz $A$ por um vetor $x$, resultando em $y = A \cdot x$, usando **MPI (Message Passing Interface)** para paralelizar o c√°lculo entre m√∫ltiplos processos.

O foco principal est√° em:

* Dividir o trabalho de forma **balanceada** (quando poss√≠vel) ou **desbalanceada** (quando necess√°rio);
* Avaliar o desempenho da aplica√ß√£o com **diferentes tamanhos de matriz** e **n√∫mero de processos**;
* Estudar o **impacto da comunica√ß√£o coletiva MPI** na efici√™ncia paralela;
* Medir corretamente o **tempo total de execu√ß√£o**, incluindo **comunica√ß√£o e c√°lculo**.

## üñ•Ô∏è Estrutura do C√≥digo

### üî® Compila√ß√£o

Compile com `mpicc`, usando otimiza√ß√£o:

```bash
mpicc -O3 -o matvec main.c
```

### ‚ñ∂Ô∏è Execu√ß√£o via SLURM

Um script SLURM automatiza os testes para diferentes tamanhos de matriz e n√∫meros de processos:

```bash
sbatch script_job.sh
```

O script testa execu√ß√µes com 2, 4, 8, 16 e 32 processos, em matrizes de tamanhos $512 \times 512$ at√© $4096 \times 4096$.

## üöÄ L√≥gica da Paraleliza√ß√£o

### 1. **Distribui√ß√£o da matriz `A`**

* A matriz √© dividida **por linhas**.
* Como $M$ (n√∫mero de linhas) pode **n√£o ser divis√≠vel** por $P$ (n√∫mero de processos), √© usada:

> ‚úÖ `MPI_Scatterv`: permite distribuir **quantidades vari√°veis** de linhas por processo.
> ‚õî `MPI_Scatter` exige divis√£o uniforme, o que causaria erro ou inefici√™ncia.

### 2. **Distribui√ß√£o do vetor `x`**

> ‚úÖ `MPI_Bcast`: utilizado para **enviar o vetor `x` completo para todos os processos**, j√° que todos precisam dele para calcular o produto local.

### 3. **C√°lculo local**

Cada processo computa o produto escalar das linhas de sua fatia da matriz com o vetor `x`, produzindo uma parte do vetor `y`.

### 4. **Recolhimento dos resultados**

> ‚úÖ `MPI_Gatherv`: usado para **reunir partes vari√°veis do vetor `y`** de volta no processo mestre.
> ‚õî `MPI_Gather` n√£o seria suficiente, pois assume tamanhos iguais de dados a serem coletados.

## üìà Gr√°ficos e An√°lise de Resultados

Os resultados foram obtidos com medi√ß√µes reais de tempo (incluindo **toda a comunica√ß√£o MPI**) e analisados estatisticamente. Os gr√°ficos abaixo foram gerados com Python, usando os dados m√©dios de 3 execu√ß√µes por configura√ß√£o.

### 1. **Speedup vs. N√∫mero de Processos**

![Speedup](./speedup-vs-numero-de-processos.png)

* Para **matrizes grandes**, o speedup aumenta com o n√∫mero de processos at√© certo ponto (\~16).
* Para **matrizes pequenas**, o uso excessivo de processos **piora o desempenho**, por causa da **sobrecarga de comunica√ß√£o**.

### 2. **Tempo de Execu√ß√£o vs. N√∫mero de Processos**

![Tempo por processo](./tempo-de-execucao-vs-numero-de-processos.png)

* Redu√ß√£o clara de tempo para matrizes grandes (2048√ó2048+).
* Com **32 processos**, h√° satura√ß√£o: o ganho computacional √© anulado pela **lat√™ncia e sincroniza√ß√£o MPI**.

### 3. **Tempo de Execu√ß√£o vs. Tamanho da Matriz**

![Tempo vs Tamanho](./tempo-de-execucao-vs-tamanho-da-matriz.png)

* O tempo cresce aproximadamente **linearmente** com o n√∫mero de elementos da matriz.
* Com mais processos, matrizes grandes s√£o resolvidas mais rapidamente.
  J√° para matrizes pequenas, o paralelismo **n√£o compensa**.

## üìå Comparativo: Por que `MPI_Scatterv` e `MPI_Gatherv`?

| Fun√ß√£o         | Quando usar                                            | Usada neste projeto? |
| -------------- | ------------------------------------------------------ | -------------------- |
| `MPI_Scatter`  | Quando todos recebem **quantidades iguais**            | ‚ùå                    |
| `MPI_Scatterv` | Quando cada processo recebe **quantidades diferentes** | ‚úÖ                    |
| `MPI_Gather`   | Quando todos enviam **quantidades iguais**             | ‚ùå                    |
| `MPI_Gatherv`  | Quando cada processo envia **quantidades diferentes**  | ‚úÖ                    |
| `MPI_Bcast`    | Para enviar o mesmo dado a todos os processos          | ‚úÖ                    |