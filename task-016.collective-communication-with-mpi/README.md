# üìò Produto Matriz-Vetor com MPI

## üß© Objetivo

Este projeto implementa a multiplica√ß√£o de uma matriz $A$ por um vetor $x$, resultando em $y = A \cdot x$, usando **MPI (Message Passing Interface)** para paralelizar o c√°lculo entre m√∫ltiplos processos.

O foco est√° em:

* Dividir o trabalho de forma balanceada ou desbalanceada.
* Avaliar o desempenho da aplica√ß√£o com diferentes tamanhos de matriz e n√∫mero de processos.
* Estudar o impacto das fun√ß√µes de comunica√ß√£o MPI utilizadas.

## üñ•Ô∏è Estrutura do C√≥digo

### üî® Compila√ß√£o

Use `mpicc` com otimiza√ß√£o para compilar o c√≥digo:

```bash
mpicc -o matvec main.c
```

### ‚ñ∂Ô∏è Execu√ß√£o via SLURM

Um script `bash` (`sbatch`) automatiza os testes com m√∫ltiplos tamanhos de matriz e diferentes quantidades de processos MPI:

```bash
sbatch script_job.sh
```

## üöÄ L√≥gica da Paraleliza√ß√£o

### 1. **Distribui√ß√£o da matriz `A`**

* A matriz √© dividida **por linhas**.
* Como `M` (n√∫mero de linhas) pode **n√£o ser divis√≠vel** por `P` (n√∫mero de processos), usamos:

> ‚úÖ `MPI_Scatterv`: permite distribuir **n√∫meros vari√°veis de linhas** por processo, diferentemente de `MPI_Scatter`, que exige que todos recebam a mesma quantidade.

### 2. **Distribui√ß√£o do vetor `x`**

> ‚úÖ `MPI_Bcast`: √© utilizado para **enviar uma c√≥pia inteira de `x` para todos os processos**, j√° que todos precisam dele para computar suas respectivas linhas.

### 3. **C√°lculo local em cada processo**

Cada processo calcula seu subconjunto de $y$ correspondente √†s suas linhas da matriz.

### 4. **Recolhimento dos resultados**

> ‚úÖ `MPI_Gatherv`: permite coletar **quantidades vari√°veis** de elementos resultantes de $y$ de volta no processo root, respeitando a distribui√ß√£o desigual feita inicialmente.

## üìà Gr√°ficos e An√°lise de Resultados

### Arquivos de sa√≠da utilizados para os gr√°ficos:

### 1. **Speedup vs. N√∫mero de Processos**

![Speedup](./speedup-vs-numero-de-processos.png)

* Para **matrizes pequenas**, o speedup **decresce** com mais processos ‚Üí a **sobreposi√ß√£o de comunica√ß√£o** e o **overhead de paraleliza√ß√£o** superam o ganho.
* Para **matrizes grandes**, h√° ganho real at√© certo ponto, mas o speedup eventualmente **satura ou diminui**.

### 2. **Tempo de Execu√ß√£o vs. N√∫mero de Processos**

![Tempo por processo](./tempo-de-execucao-vs-numero-de-processos.png)

* **Melhor desempenho** em 2, 4 ou 8 processos, dependendo do tamanho da matriz.
* Para **32 processos**, o tempo tende a crescer, indicando que o custo da comunica√ß√£o supera o benef√≠cio da divis√£o de trabalho.

### 3. **Tempo de Execu√ß√£o vs. Tamanho da Matriz**

![Tempo vs Tamanho](./tempo-de-execucao-vs-tamanho-da-matriz.png)

* O tempo cresce **linearmente com o n√∫mero de elementos**.
* Por√©m, **a escalabilidade depende do n√∫mero de processos** ‚Äî para tamanhos muito grandes, 32 processos ajudam; para tamanhos pequenos, atrapalham.

## üìå Por que usar `MPI_Scatterv` e `MPI_Gatherv`?

| Fun√ß√£o         | Quando usar                                   | Usada no c√≥digo? |
| -------------- | --------------------------------------------- | ---------------- |
| `MPI_Scatter`  | Quando todos recebem igual                    | ‚ùå                |
| `MPI_Scatterv` | Quando cada processo recebe partes diferentes | ‚úÖ                |
| `MPI_Gather`   | Quando todos enviam quantidades iguais        | ‚ùå                |
| `MPI_Gatherv`  | Quando cada processo envia partes diferentes  | ‚úÖ                |
| `MPI_Bcast`    | Para enviar o mesmo dado a todos              | ‚úÖ                |